---
jupytext:
  formats: ipynb,md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.4
kernelspec:
  display_name: Python (emd-paper)
  language: python
  name: emd-paper
---

+++ {"editable": true, "slideshow": {"slide_type": ""}, "tags": ["remove-cell"]}

---
bibliography:
    ../Fitted_model_comparison.bib
math:
    '\mspace': '\hspace{#1}'
    '\MP'    : '\mathcal{M}_{\mathrm{P}}'
    '\MRJ'   : '\mathcal{M}_{\mathrm{RJ}}'
    '\D'    : '\mathcal{D}'
    '\Bemd' : 'B_{#1}^{\mathrm{EMD}}'
    '\Bconf': 'B^{\mathrm{conf}}_{#1}'
    '\Bspec': '\mathcal{B}'
    '\EE'   : '\mathbb{E}'
    '\VV'   : '\mathbb{V}'
    '\eE'   : '\mathcal{E}'
    '\logL' : '\mathcal{l}'
    '\nN'   : '\mathcal{N}'
    '\Unif' : '\operatorname{Unif}'
    '\Poisson': '\operatorname{Poisson}'
---

+++ {"editable": true, "slideshow": {"slide_type": ""}}

# Comparison with other criteria

+++ {"editable": true, "slideshow": {"slide_type": ""}}

{{ prolog }}

%{{ startpreamble }}
%{{ endpreamble }}

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
tags: [remove-cell]
---
from Ex_UV import *
from numpy.polynomial import Polynomial
import re
import dataclasses
import matplotlib as mpl
#from myst_nb import glue
from viz import glue
from itertools import product
```

```{code-cell} ipython3
from emd_falsify import draw_R_samples
from other_criteria import FactorizedPrior, get_ppfs, R, AIC, BIC, DIC, logâ„°
```

```{code-cell} ipython3
import holoviews as hv
hv.extension("bokeh")
```

| Criterion    | Not restricted to nested models | No restriction on noise model | Not restricted to log likelihood risk | Not asymptotic | Allows singular models | Allows Bayesian models | Symmetric | Information-theoretic | Consistent | Robust viz. misspecification | Source of variability |
|--------------|---------------------------------|-------------------------------|---------------------------------------|---------------|----------------------------|------------|--------------|-----------------------|------------|----------------------|----------------|
| $\Bemd{}$    | âœ”      | âœ” | âœ”      | âœ” | âœ” | âœ” | âœ” | âœ˜ | âœ”   | âœ”    | Imperfect replications |
| WAIC         | âœ”      | âœ” | âœ”      | âœ” | âœ” | âœ” | âœ” | âœ˜ | âœ”   | âœ˜â½Â²â¾ | Posterior              |
| Bayes factor | âœ”      | âœ” | âœ˜      | âœ” | âœ˜ | âœ” | âœ” | âœ˜ | âœ”   | âœ˜â½Â²â¾ | Posterior              |
| MDL          | âœ”/âœ˜â½Â¹â¾ | âœ” | âœ”/âœ˜â½Â¹â¾ | âœ” | âœ˜ | âœ” | âœ˜ | âœ” | âœ”   | âœ˜â½Â³â¾ | ???                    |
| (BIC)        | âœ”      | âœ” | âœ˜      | âœ˜ | âœ˜ | âœ” | âœ” | âœ˜ | âœ”/âœ˜ | âœ˜    | Posterior              |
| DIC          | âœ˜      | âœ˜ | âœ˜      | âœ˜ | âœ˜ | âœ” | âœ˜ | âœ” | âœ”   | âœ˜    | Posterior              |
| AIC          | âœ˜      | âœ˜ | âœ˜      | âœ˜ | âœ˜ | âœ˜ | âœ˜ | âœ” | âœ˜   | âœ˜    | Perfect replications   |


â½Â¹â¾ Some entries are marked âœ”/âœ˜ for MDL because although the general form may not have these restrictions, they are usually required to obtain tractable approximations.  
â½Â²â¾ Bayesian methods are sometimes described as accounting for epistemic errors, but only for errors which are within the hypothesis class. So specifically not misspecification.  
â½Â³â¾ There has been at least one attempt to make MDL robust in the case of misspecified models, but it is far from standardized. It is also unclear whether it would be broadly applicable, since it depends on an ad hoc scaling of the likelihood, which itself introduces an additional hyperparameter.

+++

### Data

For this experiment we want one of the candidates to be the true model. Also, in order to compute NML complexity exactly, we want to be able to enumerate all possible datasets. This is possible thanks to how we set the problem:

- The use of Poisson noise means that possible values are discretized. Technically there are infinitely many possible values, but their probability quickly vanishes, so we need to consider only a dozen or so.
- Fixed, exact $Î»$ values: for a given dataset size $L$, we always generate data with the abscissa $Î»_k = Î¼Î»_{\mathrm{min}} + k \frac{Î»_{\mathrm{max}} - Î»_{\mathrm{min}}}{L-1}$, where $k=0, \dotsc, L-1$.

This means we can implement enumeration as follows:
1. For each $Î»_k$, determine a set of possible values for $\Bspec_k$. Below we use an interval containing 0.9998% of the probability mass. Each $\Bspec_k$ is a sequential array with equal steps.
2. A dataset is generated by picking a random value from each $\Bspec_k$: $\{(Î»_k, b_k) : b_k \sim \Bspec_k\}$.
   The total number of datasets is therefore $\prod_{k=0}^{L-1} \lvert \Bspec_k \rvert$.
3. In practice there are still an unfeasibly large number of datasets. Therefore we generate them from most to least likely: the hope is that the integrand in an NMLâ€¯integral (which in this case is a series) should correlate with the true data likelihood, and that we can truncate the series once we have enough dominant terms.

```{code-cell} ipython3
@dataclass(frozen=True)
class EnumerableDataset(Dataset):
    def num_possible_datasets(self, thresh=0.0001):
        Î», B = DataModel(Î»_min=self.Î»min, Î»_max=self.Î»max, T=self.T, phys_model="Planck")(self.L)
        sB = self.s*B
        assert sB.dimensionless; "Dimension error: s*B should be dimensionless"
        # Determine possible values for â„¬
        lower_counts = stats.poisson(sB.m).ppf(thresh).astype(int)
        upper_counts = stats.poisson(sB.m).ppf(1-thresh).astype(int)
        # Take the product of set sizes |â„¬k|
        â„¬sizes = upper_counts - lower_counts + 1
        return np.prod(â„¬sizes)
    def gen_possible_datasets(self, thresh=0.0001):
        """
        Iterator for every dataset.
        We exclude datapoints which collectively have a total probability of at most 2*thresh (default: 0.01%).
        """
        # Returns a tuple of two arrays of possibly values for Î» and B respectively.
        # To get all possible data values, take the Cartesian product of these two arrays.
        # """
        # Implementation is mirrored on poisson_noise.__call__, except that instead of
        # drawing from the Poisson, we use it to compute the range of possible values
        Î», B = DataModel(Î»_min=self.Î»min, Î»_max=self.Î»max, T=self.T, phys_model="Planck")(self.L)
        sB = self.s*B
        assert sB.dimensionless; "Dimension error: s*B should be dimensionless"
        # Determine possible values for â„¬
        lower_counts = stats.poisson(sB.m).ppf(thresh).astype(int)
        upper_counts = stats.poisson(sB.m).ppf(1-thresh).astype(int)
        # For each k, sort the â„¬ values from most to least likely
        â„¬sets = []
        for _sB, _lc, _uc in zip(sB, lower_counts, upper_counts):
            â„¬probs = stats.poisson(_sB).pmf(np.arange(_lc, _uc+1))
            â„¬vals = np.arange(_lc, _uc+1) / self.s + self.B0
            â„¬sets.append(â„¬vals[np.argsort(â„¬probs)[::-1]].m)  # Need to work with magnitudes to assign in-place below. Also assigning magnitudes is faster since it skips the unit checks
        # Go through all combinations which pick one â„¬k from each â„¬set
        â„¬arr = np.zeros_like(B) * B.units  # Pre-allocate array for efficiency
        for â„¬vals in product(*â„¬sets):
            â„¬arr.m[:] = â„¬vals
            yield Î», â„¬arr
        #for _Î», _uc in zip(Î», upper_count):
        #    yield np.broadcast_to(_Î», upper_count), np.arange(upper_count+1) / self.s + self.B0
            # for _B in np.arange(_uc+1) / self.s + self.B0:
            #     yield _Î», _B
        #return Î», np.arange(upper_count+1) / self.s + self.B0
    @cache # Add ability to strip units (not done in base class in case it would invalidate a cache)
    def get_data(self, rng=None, strip_units=False):
        Î», B = self.__call__(self.L, rng)
        return (Î».m, B.m) if strip_units else (Î», B)
```

```{code-cell} ipython3
ğ’Ÿ = EnumerableDataset(
    "comparison with other criteria",
    L   =100,
    Î»min=0.1*Î¼m,
    Î»max=2  *Î¼m,
    #Ïƒ   =4e-5 * Bunits,
    s   =(2e-2*Bunits)**-1,
    T   =data_T)
```

```{code-cell} ipython3
#data   = Dataset("", L=4000, Î»min=0.1*Î¼m, Î»max=2*Î¼m, s=(2e-3*Bunits)**-1, T=3000*K)
ğ’Ÿ_mean = replace(ğ’Ÿ, s=(4e-5*Bunits)**-1)
fig = hv.Scatter(ğ’Ÿ.get_data(strip_units=True)) * hv.Curve(ğ’Ÿ_mean.get_data(strip_units=True))
fig.opts(
    hv.opts.Curve(color="#AA0022"),
    hv.opts.Scatter(size=1.5, backend="bokeh"),
    hv.opts.Overlay(fig_inches=5, fontscale=1.5, backend="matplotlib"),
    hv.opts.Overlay(width=500, backend="bokeh")
)
```

Note that in contrast to the main setup, we consider a different regime, with wavelength window closer to the distributionâ€™s peak and lower relative noise. This makes the likelihood essentially Gaussian, up to a normalization factor.

+++

### Candidate models

Most established model comparison methods are meant to compare nested models, where preferably one of the alternatives is the true model. For criteria typically interpreted in terms of information theory, like AIC and MDL, that interpretation usually only holds when the true model is also the simplest.
Therefore to give existing criteria the fairest comparison, we setup a model comparison problem where the model alternatives are variants of the Planck radiance model augmented with a polynomial:

$$\tilde{\Bspec} \mid Î» \sim \nN\biggl(\MP(Î»\,;\, T) + \sum_{j=0}^{m-1} b_j Î»^j \;,\;\; Ïƒ^2\biggr)$$

Note that in contrast to the main setup, the difference between Poisson and Gaussian noise is not too important. So whether we fit the model using Poisson or Gaussian likelihood, the true model is recovered when $m=0$.

+++

It is worth emphasizing that in order to accommodate some criteria, we are posing a different problem than the one we actually want to solve. First, instead of comparing the Planck ($\MP$) and Rayleigh-Jeans ($\MRJ$) models, we compare some artificially augmented version of $\MP$. Second, we need to be in the regime where the Poisson is effectively a discretized Gaussian. This allows us to substitute the Poissonâ€™s discrete PMF with the Gaussianâ€™s continuous PDF for evaluating the loss. (If we tried to fit a discrete model, its PMF would vanish as soon as one data point is not in the support.) This also ensures that the asymptotic criteria, which assume that we have reached the regime where the likelihood is â€œGaussian-likeâ€, are valid. Finally, the discreteness of the data distribution nevertheless allows us to enumerate all possible datasets, a feature which is convenient for computing the NML distribution.

Making these choices of regime and data-generating model ensures no method is disadvantaged, but of course in practice a practitioner does not get to choice their data or model to fit their statistical method. 

In practice of course we would like to be able to compare models without restriction on their structure or the type of noise, as we did in [Fig. ...] comparing the Planck and Rayleigh-Jeans models with $\Bemd{}$. Bayesian methods are also agnostic to the choice of model. Bayesian methods however consider a different type of epistemic error, and in particular ignore errors due to misspecification. We illustrate this with a comparison table in [Supplementary ...]. For completeness, this table also includes values obtained with the other criteria â€“ since in practice they are often used even when they are invalid.

```{code-cell} ipython3
@dataclass(frozen=True)
class CandidateModel:
    Ïƒ : float|PintQuantity
    T : PintQuantity  # units: K
    coeffs: list[float]

    @property
    def m(self): 
        """Returns the order of the modelâ€™s spurious polynomial."""
        return len(self.coeffs)

    def __post_init__(self):
        # If parameters were given as plain floats, convert to default units
        if not isinstance(self.Ïƒ, pint.Quantity):
            object.__setattr__(self, "Ïƒ", self.Ïƒ * Bunits)   # IMO bypassing frozen dataclass in __post_init__ is acceptable
        if not isinstance(self.T, pint.Quantity):
            object.__setattr__(self, "T", self.T * ureg.kelvin)

    def Q(self, Î»_B):
        """
        The loss is computed from differences between the observed data `Î»_B`
        and predictions of the _physical_ model. The logpdf of the _observation_
        model is used to convert those differences into a loss.
        """
        Î», Bdata = Î»_B
        _, Btheory = self.physmodel(Î»)
        #return -stats.poisson.logpdf((Bdata - Btheory).m , 
        return -stats.norm.logpdf((Bdata - Btheory).m, loc=0, scale=self.Ïƒ.m)

    def obsmodel(self, Î»_B, rng=None):
        Î», Bdata = Î»_B
        return Î», Bdata + stats.norm.rvs(size=len(Bdata), loc=0, scale=self.Ïƒ.m, random_state=rng)*Bunits
    
    def physmodel(self, Î»arr: ArrayLike, rng=None) -> tuple[ArrayLike, ArrayLike]:
        if isinstance(Î»arr, tuple):  # Allow passing output of data directly
            assert len(Î»arr) == 2, "CandidateModel expects either a single array `Î»arr`, or a tuple `(Î»arr, Barr)`."
            Î»arr = Î»arr[0]
        Barr = phys_models["Planck"](Î»arr, self.T)
        if self.coeffs:
            Barr += Polynomial(self.coeffs)(Î»arr.m) * Bunits
        return Î»arr, Barr

    def gen_data(self, L, Î»min=data_Î»_min, Î»max=data_Î»_max, rng=None):
        Î»arr = np.linspace(Î»min, Î»max, L)
        return self.obsmodel(self.physmodel(Î»arr), rng=rng)
    
    def __call__(self, L, rng=None):
        return self.obsmodel(self.physmodel(Î»arr), rng=rng)
```

### Risk functional

We use the negative log likelihood as our risk functional, with a Gaussian observation model.

(For technical reasons the risk functional is defined above as a method of the candidate model. The comparison functions expect a separate functional, so `Q[Ïƒ,T,coeffs]` is a wrapper which calls `Q` from a properly parameterized model.)

```{code-cell} ipython3
class QMeta(type):
    def __getitem__(self, Î˜):
        Ïƒ, T, *coeffs = Î˜
        return self.__call__(Ïƒ, T, coeffs)
    def __call__(self, Ïƒ, T, coeffs=()):
        if len(coeffs) == 1 and isinstance(coeffs[0], (list, tuple)):
            coeffs = coeffs[0]
        #return Q(Ïƒ, T, coeffs)
        model = CandidateModel(Ïƒ, T, coeffs)
        return model.Q
#@dataclass
class Q(metaclass=QMeta):
    pass
```

### Fitting function
Given some observed data and the order of the extra polynomial, returns a tuple `(Ïƒ, T, coeffs)` of fitted parameters.

Parameters are fitted by minimizing $Q$, subject to some mild regularization on $Ïƒ$ and $T$.

```{code-cell} ipython3
def fitÎ˜(Î»â„¬, m):
    log2_T0    = np.log2(data_T.m)
    priorT_std = 12  # |logâ‚‚ 4000 - logâ‚‚ 5000| â‰ˆ 12
    def f(Î˜tilde, Î»â„¬=Î»â„¬):
        log2Ïƒ, log2T, *coeffs = Î˜tilde
        Ïƒ = 2**log2Ïƒ; T = 2**log2T
        risk = Q(Ïƒ, T, coeffs)(Î»â„¬).mean()
        priorÏƒ = 2**(-log2Ïƒ/128)  #â€¯Soft floor on Ïƒ so it cannot go too low
        priorT = (log2T - log2_T0)**2 / (2*priorT_std**2)  
        return risk + priorÏƒ + priorT

    res = optimize.minimize(f, np.r_[np.log2([1e-4, data_T.m]), [0]*m], tol=1e-5)
    log2Ïƒ, log2T, *coeffs = res.x
    Ïƒ = 2**log2Ïƒ; T = 2**log2T

    return Ïƒ, T, tuple(coeffs)
```

### Bayesian prior

```{code-cell} ipython3
rv = stats.loguniform(2**9, 2**14)
```

```{code-cell} ipython3
Ï€ = FactorizedPrior(["loguniform", "loguniform"],
                    [(2**9, 2**14), (1000, 5000)],
                    rng="prior - compare - models")
```

### NML distribution

+++

The model complexity is computed as

$$\mathrm{COMP} = \log \sum_{z^n \in \D} \max_Î˜( \logL(Î˜)Ï€(Î˜) )$$

We can do this as follows:

1. $\bar{l} \leftarrow [\,]$
2. __for__ $z^n \in \D$:
    1. $Ïƒ, T, m \leftarrow \mathtt{fit}(z^n)$
    2. $l_k \leftarrow l(z^n; Ïƒ, T)$
    3. $\bar{l}.\mathtt{append}(l_k)$
3. __return__ $\mathtt{logsumexp}(\bar{l} = [l_1, l_2, \dots])$

There are a _lot_ of datasets in $\D$, but if we draw them such that the fitted likelihoods $\bar{l}.\mathtt{append}(l(z^n; Ïƒ, T))$ are in decreasing order, then we can truncate the sum once we have enough dominant terms. What we actually do is draw them in decreasing order of the true likelihood, but this should be close enough.

To upper bound the remaining error, suppose we already have $M$ terms and $M'$ remain (so $M + M' = \lvert \D \rvert$). Suppose also that $l = l_{M'}$ is the most recently computed log likelihood (and therefore also approximately the smallest). All the remaining datasets should not, on average, have higher log likelihood. Then a crude upper bound for the remaining contribution is $M e^l$. It then follows that the model complexity is bounded by

$$\mathrm{COMP} = \log \Bigl(\sum_{k \in M + M'} e^{l_k} \Bigr) \leq \log \Bigl(\sum_{k \in M} e^{l_k} + M' e^l\Bigr) \leq \log \Bigl(\sum_{k \in M} e^{l_k} \Bigr) + l\log M',$$

where $\log \Bigl(\sum_{k \in M} e^{l_k} \Bigr) \approx \mathrm{COMP}$ is the current estimate of the complexity. Therefore it should be safe to truncate the sum when

$$\frac{l\log M'}{\log \Bigl(\sum_{k \in M} e^{l_k} \Bigr)} \approx \frac{l\log M'}{\mathrm{COMP}} < Îµ \,,$$

where $Îµ$ is say 0.01.

```{code-cell} ipython3
from scipy.special import logsumexp
```

```{code-cell} ipython3
ğ’Ÿ = EnumerableDataset(
    "comparison with other criteria",
    L   =10,
    Î»min=0.1*Î¼m,
    Î»max=2  *Î¼m,
    #Ïƒ   =4e-5 * Bunits,
    s   =(2e-2*Bunits)**-1,
    T   =data_T)
```

```{code-cell} ipython3
Diter = ğ’Ÿ.gen_possible_datasets(0.1)
lbar = np.zeros(400)
comp = -np.inf  # Equivalent to initializing a running total with zero
Îµ = 0.01
M = 0
```

```{code-cell} ipython3
nğ’Ÿ = ğ’Ÿ.num_possible_datasets(0.1)
```

```{code-cell} ipython3
while M < nğ’Ÿ:
    
    lbar[:] = 0
    for i in range(lbar.size):
        (_Î»,_â„¬) = next(Diter)
        Ïƒ, T, coeffs = fitÎ˜((_Î»,_â„¬), 0)
        lbar[i] = -Q(Ïƒ, T, coeffs)((_Î»,_â„¬)).sum()
        M += 1
        
    comp = np.logaddexp(comp, logsumexp(lbar))
    
    if np.log(nğ’Ÿ - M) * lbar.min() / comp < Îµ:
        break
```

## Test criterion evaluations

```{code-cell} ipython3
Î¸Ë† = fitÎ˜(ğ’Ÿ.get_data(), m=1)
ğ“œ = CandidateModel(*Î¸Ë†)
```

```{code-cell} ipython3
R(Q, Î¸Ë†, ğ’Ÿ)
```

```{code-cell} ipython3
AIC(Q, Î¸Ë†, ğ’Ÿ)
```

```{code-cell} ipython3
BIC(Q, Î¸Ë†, ğ’Ÿ)
```

```{code-cell} ipython3
DIC(Q, Î¸Ë†, Ï€, ğ’Ÿ)
```

```{code-cell} ipython3
logâ„°(Q, Ï€, ğ’Ÿ)
```

```{code-cell} ipython3
mixed_ppf, synth_ppf = get_ppfs(ğ“œ, ğ’Ÿ, rng=utils.get_rng("synth ppf - compare - models"))
draw_R_samples(mixed_ppf, synth_ppf, 0.5)
```

## Comparison figure

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
tags: [active-ipynb, remove-cell]
---
hv.extension("matplotlib", "bokeh")
```

Note: We donâ€™t change the datasetâ€™s seed, so there is correlation in the data, but not exactly the same $(x_i, y_i)$ pairs are drawn. (Because the $x_i$ are distributed evenly across $[Î»_{min}, Î»_{max}]$.)

```{code-cell} ipython3
# scores = {crit: {m: [] for m in range(4)}
#           for crit in ["AIC", "BIC", "EMD"]}
scores = {}
#scores = []
for m in tqdm([0, 1], desc="m"):
    for L in tqdm([2**n for n in (5, 6, 7, 8, 9, 10)], desc="L"):
        ğ’Ÿ = replace(ğ’Ÿ, L=L)
        Î¸Ë† = fitÎ˜(ğ’Ÿ.get_data(), m=m)
        ğ“œ = CandidateModel(*Î¸Ë†)

        scores[("R", m, L)] = R(Q, Î¸Ë†, ğ’Ÿ)
        scores[("AIC", m, L)] = AIC(Q, Î¸Ë†, ğ’Ÿ)
        scores[("BIC", m, L)] = BIC(Q, Î¸Ë†, ğ’Ÿ)
        #scores[("logE", m, L)] = logâ„°(Q, Ï€, ğ’Ÿ)
        mixed_ppf, synth_ppf = get_ppfs(ğ“œ, ğ’Ÿ, rng=utils.get_rng("synth ppf - compare - models"))
        scores[("Rdist", m, L)] = draw_R_samples(mixed_ppf, synth_ppf, 0.5)
        
```

```{code-cell} ipython3
relscores = []
for key, s in scores.items():
    crit, m, L = key
    # For scores with arbitrary zero, compute relative values
    if crit in {"AIC", "BIC"}: s -= scores[crit, 0, L]
    # For Rdist, unpivot/convert to a narrow format
    if crit == "Rdist":
        relscores.extend([(crit, m, L, _s) for _s in s])
    else:
        relscores.append((crit, m, L, s))
```

```{code-cell} ipython3
tab = hv.Table(relscores, kdims=["criterion", "m", "L"], vdims=["score"])
```

```{code-cell} ipython3
tab.criterion
```

```{code-cell} ipython3
hv.Violin(scores["Rdist", 0, 32], vdims="score")
```

```{code-cell} ipython3
tab
```

```{code-cell} ipython3
tab.select(criterion="Rdist", L=32, m=0).to.violin(kdims="L", vdims="score")
```

```{code-cell} ipython3
tab.select(criterion="Rdist", L=32, m=0).to.distribution()
```

```{code-cell} ipython3
hv.output(
    tab.select(criterion="Rdist", m=0).to.violin(kdims="L", vdims="score"),
    backend="bokeh"
    )
```

```{code-cell} ipython3
tab.select(criterion="Rdist", m=0, L=32);
```

```{code-cell} ipython3
ov = tab.select(criterion="AIC").to.curve(kdims="L", vdims="score").overlay("m")
#ov.opts(logx=True, logy=True)
```

```{code-cell} ipython3
ov
```

```{code-cell} ipython3
ds = hv.Dataset(scores, kdims=["criterion", "m", "L"])
```

```{code-cell} ipython3
hv.Dataset?
```

```{code-cell} ipython3
hm = hv.HoloMap(scores, kdims=["criterion", "m", "L"])
```

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
tags: [remove-cell]
---
## logsumexp using natural log ##
# log = np.log
# exp = np.exp
# logsumexp = scipy.special.logsumexp
# base_e_to_x = 1
# base_x_to_10 = base_e_to_10
## logsumexp using pow 2 ##
from functools import reduce
log = np.log2                  # If we use log transforms for numerical stability, log2 makes
exp = lambda x: 2**x           # more sense: faster and more stable
def logsumexp(x, axis=0): assert axis==0; return np.logaddexp2.reduce(x)  # Faster
#def logsumexp(x, axis=0): assert axis==0; return reduce(np.logaddexp2, x)  # Lower memory
base_e_to_x = np.log2(np.exp(1))
base_x_to_10 = np.log10(2)

array = np.array
replace = dataclasses.replace
```

## Notebook parameters

+++ {"editable": true, "slideshow": {"slide_type": ""}}

For the comparison with other criteria, we use variations of the dataset shown in {numref}`fig_UV_setup`:

- We add an intermediate dataset size `L_med`.
- We increase the upper wavelength range so data go into the microwave range.
- We use three different noise levels (with the middle noise closest to the data in {numref}`fig_UV_setup`.
- We use two bias conditions: unbiased, or bias equal to {numref}`fig_UV_setup` / 8.
  (We need to reduce the bias because at large wavelengths the radiance is smaller.)

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
L_list = [L_small, L_med, L_large]
Lá‘Š = 2**12
#s_list = 2.**array([12, 16, 20]) * Bunits**-1  # data_noise_s is about 2**16.5
s_list = 2.**array([9, 12, 14]) * Bunits**-1

table_Î»windows = [(6*Î¼m, 20*Î¼m),
                  #(20*Î¼m, 1000*Î¼m),
                  #(data_Î»_min, data_Î»_max)
                 ]
table_T    = data_T
table_B0   = data_B0/8

path_output_table = config.paths.figures/"table_criterion_comparison"

dataset_list = [Dataset("Criterion comparison", L, Î»min, Î»max, s, T=table_T, B0=B0)
                for s in s_list
                for L in L_list
                for (Î»min, Î»max) in table_Î»windows
                for B0 in (table_B0, 0*Bunits)]
```

+++ {"editable": true, "slideshow": {"slide_type": ""}}

To provide a reference, we compute for each dataset the ratio between the variance (controlled by $s$) and the maximum value of the radiance. This gives a sense of how the noise compares with the scale of the data.

The variance is actually $Î»$-dependent;â€¯to give a single value, we average the expression for the variance given in the paper over $Î»$:

$$\text{noise sd} = \sqrt{ \EE_Î» \Bigl[ \VV\bigl[\Bspec(Î»;T)\bigr]\Bigr]} = \sqrt{\EE_Î» \left[ \frac{\Bspec_{\mathrm{P}}(Î»;T)}{s}  \right]} $$

We compare this against $\max_Î»\Bigl(\Bspec_{\mathrm{P}}(Î»;T)\Bigr) = \Bspec_{\mathrm{P}}(Î»_{min};T) + \Bspec_0$:

$$\text{noise fraction} := \frac{\text{noise sd}}{\max_Î»\Bigl(\Bspec_{\mathrm{P}}(Î»;T)\Bigr)} $$

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
noise_fraction = {}
for ğ’Ÿ in dataset_list:
    Î», B = ğ’Ÿ.get_data()
    Ba = ğ’Ÿ.data_model[1](ğ’Ÿ.L)[1]
    noise_sd = np.sqrt((Ba/ğ’Ÿ.s).mean())
    noise_fraction[ğ’Ÿ.B0, ğ’Ÿ.s] = noise_sd / (Ba+ğ’Ÿ.B0).max()
```

+++ {"editable": true, "slideshow": {"slide_type": ""}}

## Criteria definitions

+++ {"editable": true, "slideshow": {"slide_type": ""}}

For comparability, we write all criteria as ratios of probabilities, so a criterion $B^C_{AB}$ is understood as

> Model $A$ is $B^C_{AB}$ times more probable than model $B$.

where $C$ can be â€œmodel evidenceâ€, â€œrelative likelihoodâ€, etc. Thus, if $P(A)$ is the â€œprobability of model $A$ and $P(B)$ the â€œprobability of model $B$, then a criterion corresponds to
$$B^C_{AB} = \frac{P(A)}{P(B)} \qquad \leftrightarrow \qquad \log B^C_{AB} = \log P(A) - \log P(B) \,.$$  (eq_conceptual-probability-ratio)

+++

:::{margin}
`Criterion` class takes two arguments â€“ `lognum` and `logdenom` â€“  which correspond respectively to $\log P(A)$ and $\log P(B)$ in {eq}`eq_conceptual-probability-ratio`.
__Values must be provided in base $e$.__ (I.e. we assume natural logarithms.)
A `Criterion` object has two attributes, `ratio` and `logratio`, the latter of which is __in base 10__.
:::

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
@dataclasses.dataclass
class Criterion:
    lognum  : float
    logdenom: float
    @property
    def ratio(self): return exp(self.lognum - self.logdenom)
    @property
    def logratio(self): return (self.lognum - self.logdenom)*base_x_to_10
```

+++ {"editable": true, "slideshow": {"slide_type": ""}}

Criteria are often presented this way because the notion of â€œprobability of a modelâ€ is ill-defined, and the quantities $P(A)$ and $P(B)$ often diverge or go to zero â€“ making the ratio the only quantity with some degree of stability. Of course, ratios of the form $\tfrac{0}{0}$ or $\tfrac{\inf}{\inf}$ are known to lead to all kinds of pathologies, which is what we want to illustrate with this section.

The $\Bemd{}$ attempts to resolve this by defining a proper probability which does not require a ratio to obtain a finite number:
$$\Bemd{} = P(R_A < R_B) \,.$$
We consider this a better assessment quantity, less prone to over/underconfidence than a ratio with diverging denominator. However, since other criteria have no equivalent form, for the purpose of comparability, we will convert $\Bemd{}$ into a ratio-like quantity $\underline{B}^{\mathrm{EMD}}$ (we use $\approx^*$ to denote â€œconceptual equivalenceâ€, rather a strict mathematical equality):
$$\begin{align}
\Bemd{} &\approx^* \frac{P(A)}{P(A) + P(B)} \\
\therefore \; \underline{B}^{\mathrm{EMD}} &\approx^* \frac{P(A)}{P(B)} \approx^* \frac{\Bemd{}}{1 - \Bemd{}} \\
\log \underline{B}^{\mathrm{EMD}} &\approx^* \log {\Bemd{}} - \log \bigl(1 - \Bemd{}\bigr) \,.
\end{align}$$

+++ {"editable": true, "slideshow": {"slide_type": ""}}

:::{note}
:class: margin
$\Bemd{}$ does not require specifying priors on the parameters or reserving test data.
:::

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
@memory.cache
def Bemd(ğ’Ÿ):
    #c = 2**-1 if ğ’Ÿ.Î»max == 30*Î¼m else 2**0
    c = c_chosen
    _mixed_ppf, _synth_ppf = get_ppfs(ğ’Ÿ)
    _Bemd = emd.Bemd(_mixed_ppf["Planck"], _mixed_ppf["Rayleigh-Jeans"],
                     _synth_ppf["Planck"], _synth_ppf["Rayleigh-Jeans"],
                     c=c, res=8, M=128,
                     progbarA=None, progbarB=None)
    return Criterion(log(_Bemd), log(1-_Bemd))
```

+++ {"editable": true, "slideshow": {"slide_type": ""}}

| Symbol | Variable | Meaning |
|--------|----------|---------|
|$L$     | `L`  | Number of data samples used to fit the model. |
|$L'$    | `Lá‘Š`      | Number of *new* data samples used only to test the model |
|$\mathcal{D}$ | `ğ’Ÿ` | Dataset |
|$\hat{Ïƒ}$, $\hat{T}$ | `ÏƒË†`, `TË†` | Maximum likelihood parameters |

+++ {"editable": true, "slideshow": {"slide_type": ""}}

$\logL$: Log likelihood function
~ Recall that the likelihood is a function of model parameters, so we can write
  $$\logL_a(Ïƒ, T)  := \sum_{i=1}^L -Q_{a}(\Bspec_i \mid Î»_i, Ïƒ, T) \,,$$
  since we defined the loss $Q$ to be the negative log probability.

  To assign a likelihood to a model, some criteria use $\max_{Ïƒ, T} \logL_a(Ïƒ, T)$; i.e. they evaluate the at the fitted parameters. In the following we denote this $\logL_a(\hat{Ïƒ}_a, \hat{T}_a)$.

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
def lâ‚(a, Ïƒ, T, ğ’Ÿ):
    return -Q(a, Ïƒ=array(Ïƒ).reshape(-1,1), T=array(T).reshape(-1,1)
             )(ğ’Ÿ.get_data()).sum().squeeze() * base_e_to_x
```

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
tags: [remove-cell]
---
def lâ‚(a, Ïƒ, T, ğ’Ÿ):  # This version can deal with larger datasets, by splitting the data into manageable chunks
    Î», â„¬ = ğ’Ÿ.get_data()
    Î” = 2**12
    return sum(
        -Q(a, Ïƒ=array(Ïƒ).reshape(-1,1), T=array(T).reshape(-1,1)
          )((Î»[i*Î”:(i+1)*Î”], â„¬[i*Î”:(i+1)*Î”])).sum().squeeze() * base_e_to_x
        for i in range(int(np.ceil(len(Î»)/Î”)))
    )
```

+++ {"editable": true, "slideshow": {"slide_type": ""}}

:::{note}
:class: margin

When we check the fitted temperatures, they are all in the range 3200-4100 K. So these are reasonable datasets
:::

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
tags: [hide-input]
---
ÏƒË† = {}; TË† = {}
def f(log2Ïƒ_T, a, ğ’Ÿ): Ïƒ, T = 2**log2Ïƒ_T; return -lâ‚(a, Ïƒ, T, ğ’Ÿ)
class res:
    success = False
for ğ’Ÿ in dataset_list:
    for a in ["Rayleigh-Jeans", "Planck"]:
        _ğ’Ÿ = replace(ğ’Ÿ, L=L_med)  # Always use a reasonable number of samples to fit Ïƒ and T => Too large can fail, and too small adds a different source variability than what we want to compare
        s0 = np.sqrt(_ğ’Ÿ.get_data()[1].mean()/ğ’Ÿ.s).m  # Initial s guess with moment matching
        res = optimize.minimize(f, np.log2([s0, 4000]), (a, _ğ’Ÿ), tol=1e-3); #assert res.success
        if not res.success: print(np.log2(D.s.m))
        ÏƒË†[(a,ğ’Ÿ)] = 2**res.x[0]; TË†[(a,ğ’Ÿ)] = 2**res.x[1]
```

+++ {"editable": true, "slideshow": {"slide_type": ""}}

$R$: Expected risk
~ In our paper we treat the expected risk as a random variable, and the EMD distribution is a way to induce a distribution on $R$. In this section however we mean the expected risk in the usual sense, where it is a scalar obtained by averaging the loss on test samples:
  $$R_a := \frac{1}{L'} \sum_{j=1}^{L'} Q_{a}(\Bspec_j \mid Î»_j, \hat{Ïƒ}_{a}, \hat{T}_a) \approx -\frac{1}{L} \logL_a(\hat{Ïƒ}_{a}, \hat{T}_a) \,.$$
  Note that the log likelihood is evaluated on *training* samples, while the expected risk is evaluated on *test* samples. That said, because the models are very simple and we can make $L$ and $L'$ as large as we want, the difference between the two sides of this equation should be negligeable.

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
@cache
def R(a,ğ’Ÿ):
    return Q(a, Ïƒ=ÏƒË†[a,ğ’Ÿ], T=TË†[a,ğ’Ÿ])(replace(ğ’Ÿ, purpose="test", L=Lá‘Š).get_data()).mean() * base_e_to_x
```

+++ {"editable": true, "slideshow": {"slide_type": ""}}

$\eE$: Model evidence
~ The Bayesian model evidence is obtained by integerating the posterior. For this we need to set priors on the parameters $T$ and $Ïƒ$; a common practice is to use â€œuninformative priorsâ€, to â€œlet the data speakâ€ {cite:p}`gelmanHolesBayesianStatistics2021`. (In other words, we choose broad priors which let the posterior concentrate near the likelihood.) For this example we use the following priors:
  $$\begin{aligned}
    Ï€\Bigl(\log \frac{Ïƒ}{[Ïƒ]}\Big) &\sim \Unif(\log 2^9, \log 2^{14})   &&\quad& Ï€_2\Bigl(\log \frac{Ïƒ}{[Ïƒ]}\Big) &\sim \Unif(\log 2^9, \log 2^{10})\\
    Ï€\Bigl(\log \frac{T}{[T]}\Bigr) &\sim \Unif(\log 1000, \log 5000) &&\quad& Ï€_2\Bigl(\log \frac{T}{[T]}\Bigr) &\sim \Unif(\log 3900, \log 4100)\\
    Ï€(Î») &\sim \Unif(20 \mathrm{Î¼m}, 30 \mathrm{Î¼m})
  \end{aligned}$$
  (The â€œpriorâ€ over $Î»$ reflects that we sample the wavelengths at regular intervals betwen $20 \mathrm{Î¼m}$ and $30 \mathrm{Î¼m}$. Thanks to the choice of a uniform distribution it drops out from the calculations.) For simplicity and to emulate modelling errors, we donâ€™t define a prior over the bias $\Bspec_0$: we take all candidate models to be *unbiased*.
  :::{admonition} Sensitivity to the choice of prior
  :class: note dropdown
  The use of uninformative priors is well motivated in the context of *inference*, since with enough data the posterior becomes insensitive to the choice of prior. This is **not** the case however for the computation of the evidence, where sensitivity to the prior is strong for all dataset sizes $L$. Thus we could get numerically different results with different choices of the prior. However at least in this example, simply tightening the priors (i.e. using $Ï€_2$ above) results in numerical differences on the order of 1%.
  :::

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
tags: [hide-input]
---
# To use joblib.Memory, we need objects which can be hashed & pickled consistently
# Stats distributions donâ€™t do this, but we can store their arguments instead
@dataclasses.dataclass(frozen=True)
class Prior:
    distname: str
    args: tuple
    rng: int|str|tuple[int|str]
    @property
    def rv(self):
        rv = getattr(stats, self.distname)(*self.args)
        rv.random_state = utils.get_rng(self.rng)
        return rv
    @property
    def rvs(self): return self.rv.rvs
    @property
    def pdf(self): return self.rv.pdf
    @property
    def logpdf(self): return self.rv.logpdf
```

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
Ï€1logÏƒ = Prior("uniform", (log(2**9), log(2**14)), "prior Ïƒ")
Ï€2logÏƒ = Prior("uniform", (log(2**9), log(2**10)), "prior Ïƒ")
Ï€1logT = Prior("uniform", (log(1000), log(5000)), "prior T")
Ï€2logT = Prior("uniform", (log(3900), log(4100)), "prior T")
```

+++ {"editable": true, "slideshow": {"slide_type": ""}}

  With these choices, the *evidence* for model $a$ can be written
  $$\eE_a = \iint \! dÏƒ dT \, Ï€(Ïƒ) Ï€(T) \; p\bigl(\{Î»_i, \Bspec_i\}_{i=1}^L \mid a, Î», Ïƒ, T\bigr)  \,,$$
  with $p(\{Î»_i, \Bspec_i\}_{i=1}^L \mid a, Ïƒ, T)$ given by the Gaussian observation model and $Ï€(Î»_i) = \frac{1}{10}$:
  $$\begin{aligned}
  p(\{Î»_i, \Bspec_i\}_{i=1}^L \mid a, Ïƒ, T) &= \prod_{i=1}^L \frac{1}{\sqrt{2Ï€}Ïƒ} \exp\left(-\frac{\bigl(\Bspec_i-\hat{\Bspec}_a(Î»;T)\bigr)^2}{2Ïƒ^2} \right) Ï€(Î»_i) \,, \\
  &= \exp\left[-\frac{L}{2} \log(2Ï€Ïƒ^2) - \sum_{i=1}^L \frac{\bigl(\Bspec_i-\hat{\Bspec}_a(Î»;T)\bigr)^2}{2Ïƒ^2} + L\log \frac{1}{10} \right]\,, \\
  &= \exp\left[ \logL_a\Bigl(Ïƒ, T\Bigr) + L \log \frac{1}{10} \right] \,.
  \end{aligned}$$
  The evidence in this example is therefore computed with a double integral, which can be done directly with `scipy.dblquad` if $L$ is small. When $L$ is large however, the probability is too small and we run into numerical underflow; in that case we can resort to evaluating the integral by generating a large number $M$ of prior samples:
  $$\begin{aligned}
  \eE_a &\approx \sum_j p(\{Î»_i, \Bspec_i\}_{i=1}^L \mid a,  Ïƒ_j, T_j) \qquad\text{where}\quad (Ïƒ_j, T_j) \sim  Ï€(Ïƒ) Ï€(T) \,, \\
  &\approx \left(\frac{1}{10}\right)^L \sum_j \exp\left[ \logL_a\Bigl(Ïƒ_j, T_j\Bigr) \right] \\
  \log \eE_a &\approx -L\log10 + \log\left[ \sum_j \exp \biggl(\logL_a\Bigl(Ïƒ_j, T_j\Bigr) \biggr)\right] \,.
  \end{aligned}$$

We can then replace the probabilities with their logs and use `logsumexp` to get
$$\log \eE_a \approx -L\log10 + \mathtt{scipy.special.logsumexp}\left(\logL_a^{(1)}, \logL_a^{(2)}, \dotsc \right)\mathtt{.mean()}\,,$$
where $\logL_a^{(j)} = \logL_a(Ïƒ_j, T_j)$, $(Ïƒ_j, T_j) \sim  Ï€(Ïƒ) Ï€(T)$ and $j = 1, 2, \dotsc, L_{\eE}$.

```{code-cell} ipython3
Lâ„° = 2**14   # Number of Monte Carlo samples for integral. Use giant number so result is effectively exact
```

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
@memory.cache
def logâ„°(a, ğ’Ÿ, Ï€logÏƒ, Ï€logT, Lâ„°=Lâ„°):
    rng = utils.get_rng("uv", "evidence")
    return logsumexp([lâ‚(a, Ïƒj, Tj, ğ’Ÿ)  # NB: We omit the `-L log 10`, which evenually drops out
                      for Ïƒj, Tj in zip(exp(Ï€logÏƒ.rvs(Lâ„°, random_state=rng)), exp(Ï€logT.rvs(Lâ„°, random_state=rng)))
                     ])
```

+++ {"editable": true, "slideshow": {"slide_type": ""}}

$\mathrm{elpd}$: Expected log pointwise predictive density, WAIC, LOO
~ The expected log pointwise predictive density ($\mathrm{elpd}$) on unseen data is often considered a gold standard when it comes to comparing models. Directly estimating the elpd requires putting aside a large amount of data for testing, which is rarely feasible; hence approximations have been developed, like the widely applicable information criterion (WAIC) and leave-one-out (LOO) cross-validation {cite:p}`vehtariPracticalBayesianModel2017`. In this example however we can generate data at will, so there is no need for approximations: we can compute the $\mathrm{elpd}$ directly.

  In the following we use $Î»_i$, $\Bspec_i$ to denote the original data points used to fit the model, and $Î»_j'$, $\Bspec_j'$ (with $j \in \{1,\dotsc,L'\}$) to denote the *new* data points on which we evaluate the $\mathrm{elpd}$. The posterior over parameters is denoted $p^*(T, Ïƒ \mid \{Î»_i, \Bspec_i\}_{i=1}^L\bigr)$.
  $$\begin{aligned}
    \mathrm{elpd}_a
    &\approx \frac{1}{L'}\sum_{j=1}^{L'} \log p\bigl(Î»_j, \Bspec_j' \mid \{Î»_i, \Bspec_i, a\}\bigr) \\
    &\approx \frac{1}{L'}\sum_{j=1}^{L'} \log \Bigl[
        \iint \!dTdÏƒ \; p\bigl(Î»_j, \Bspec_j' \mid T, Ïƒ, a\bigr) \, p^*\bigl(T, Ïƒ \mid \{Î»_i, \Bspec_i\}, a\bigr)
        \Bigr] \\
    &\approx \frac{1}{L'}\sum_{j=1}^{L'} \log \left[
        \iint \!dTdÏƒ\; p\bigl(Î»_j, \Bspec_j' \mid T, Ïƒ, a\bigr) \, \frac{p\bigl(\{Î»_i, \Bspec_i\} \mid T, Ïƒ, a \bigr)\, Ï€(T)Ï€(Ïƒ)}{\int \!dTdÏƒ\; p\bigl(\{Î»_i, \Bspec_i\} \mid T, Ïƒ, a \bigr)\, Ï€(T)Ï€(Ïƒ)}
        \right] \\
    &\approx \frac{1}{L'}\sum_{j=1}^{L'} \log \left[
        \iint \!dTdÏƒ\; p\bigl(Î»_j, \Bspec_j' \mid T, Ïƒ, a\bigr) \, \frac{e^{\logL_a(Ïƒ, T)} \, Ï€(Î»)Ï€(T)Ï€(Ïƒ)}{\eE_a}
        \right] \qquad \left(Ï€(Î») \equiv \frac{1}{10}\right) \\
    &\approx \frac{1}{L'}\sum_{j=1}^{L'} \log \left[
        \iint \!dTdÏƒ\; p\bigl(Î»_j, \Bspec_j' \mid T, Ïƒ, a\bigr) \, e^{\logL_a(Ïƒ, T)} \, Ï€(T)Ï€(Ïƒ)
        \right] - \log \eE_a - \log 10
  \end{aligned}$$
  The integrand involves only probabilities of single samples, and so can be computed directly with `scipy.dblquad`.
  :::{margin}
  As it turns out, even though direct integration is feasible, it is still about 100x slower than averaging over samples and using `logsumexp`.
  :::
  
  Since $Ï€(T)$ and $Ï€(Ïƒ)$ are uniform in log space, we can further simplify the integral by a change of variables:

  $$\begin{aligned}
  \mathrm{elpd}_a
    &\approx \frac{1}{L'}\sum_{j=1}^{L'} \log \Biggl[
        \iint \!dTdÏƒ\; \exp\Bigl( \underbrace{\log p\bigl(Î»_j, \Bspec_j' \mid T, Ïƒ, a\bigr)}_{\eqqcolon\, -Q_a(Î»_j, \Bspec_j; Ïƒ, T)} + \logL_a(Ïƒ, T) \Bigr) \, Ï€(T)Ï€(Ïƒ)
        \Biggr] - \log \eE_a - \log 10 \\
    &\approx \frac{1}{L'}\sum_{j=1}^{L'} \log \Biggl[
       \underbrace{Ï€(\log T)}_{=(\log 5)^{-1}} \; \underbrace{Ï€(\log Ïƒ)}_{=(\log 16)^{-1}} \; \int_{\log 1000}^{\log 5000}\mspace{-48mu} d(\log T) \int_{\log 2}^{\log 32}\mspace{-32mu} d(\log Ïƒ)\; \exp\Bigl( \logL_a(e^{\log Ïƒ}, e^{\log T}) - Q_a(Î»_j, \Bspec_j; e^{\log Ïƒ}, e^{\log T})\Bigr)
        \Biggr] - \log \eE_a - \log 10 \\
    &\approx \frac{1}{L'}\sum_{j=1}^{L'} \log \Biggl[
       \int_{\log 1000}^{\log 5000}\mspace{-48mu} d(\log T) \int_{\log 2}^{\log 32}\mspace{-32mu} d(\log Ïƒ)\; \underbrace{\exp\Bigl( \logL_a(e^{\log Ïƒ}, e^{\log T}) - Q_a(Î»_j, \Bspec_j; e^{\log Ïƒ}, e^{\log T})\Bigr)}_{\eqqcolon h(\log Ïƒ, \log T)}
        \Biggr] - \log \eE_a - \log 10 - \log\log 5 - \log \log 16
\end{aligned}$$

  The $\mathrm{elpd}$ is closely related to the expected risk $R$; in fact, if we define $Q$ to be the log *posterior* instead of the log *likelihood*, it becomes equivalent.

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
@memory.cache
def elpd(a, ğ’Ÿ, Ï€logÏƒ, Ï€logT):
    Lâ„° = 2**14   # Number of Monte Carlo samples for integral. Use giant number so result is effectively exact
    rng = utils.get_rng("uv", "elpd")
    def h(Ïƒ, T, Î»_â„¬, a=a, ğ’Ÿ=ğ’Ÿ): return lâ‚(a, Ïƒ, T, ğ’Ÿ) - Q(a, Ïƒ, T)(Î»_â„¬)*base_e_to_x
    Ïƒarr = exp(Ï€logÏƒ.rvs(Lâ„°, random_state=rng))
    Tarr = exp(Ï€logT.rvs(Lâ„°, random_state=rng))
    Î»_test, â„¬_test = replace(ğ’Ÿ, L=Lá‘Š, purpose="test").get_data()
    return logsumexp(h(Ïƒarr, Tarr, (Î»_test[:,None], â„¬_test[:,None])), axis=0
                     ).mean() - logâ„°(a, ğ’Ÿ, Ï€logÏƒ, Ï€logT)  # NB: We omit constant terms
```

+++ {"editable": true, "slideshow": {"slide_type": ""}}

:::{dropdown} Slower version using `dblquad`
```python
def elpd(a, L):
    dblquad = scipy.integrate.dblquad
    def h(logÏƒ, logT, Î»_â„¬, a=a, L=L):
        return exp( lâ‚(a, L, Ïƒ:=exp(logÏƒ), T:=exp(logT)) - Q(a, Ïƒ, T)(Î»_â„¬) )
    Î»_test, â„¬_test = replace(ğ’Ÿ, L=Lá‘Š, purpose="test").get_data()
    return sum( log( dblquad(h, log(1000), log(5000), log(2), log(32), [(Î»j, â„¬j)]) )
                for Î»j, â„¬j in tqdm(zip(Î»_test, â„¬_test), total=Lá‘Š)) / Lá‘Š \
           - logâ„°(a, L)  # We omit the constant terms, which cancel out in the ratio
```
:::

+++ {"editable": true, "slideshow": {"slide_type": ""}}

$B^l$: Relative likelihood / AIC
~ Computing the ratio of likelihoods is equivalent to the difference of *log* likelihoods:
  $$\log B^l = \logL_{\mathrm{P}}(\hat{Ïƒ}_{\mathrm{P}}, \hat{T}_{\mathrm{P}}) - \logL_{\mathrm{RL}}(\hat{Ïƒ}_{\mathrm{P}}, \hat{T}_{\mathrm{P}}) \,.$$
~ The Akaike information criterion (AIC) for a model with $k$ parameters reads
  $$AIC_a = 2k - 2\logL_a(\hat{Ïƒ}_a, \hat{T}_a) \,.$$
  Since the Rayleigh-Jeans and Planck models both have the same number of parameters, this is equivalent to the likelihood ratio (up to a factor of 2).

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
@cache
def Bl(ğ’Ÿ): return Criterion(
    lâ‚("Planck",        ÏƒË†[("Planck",ğ’Ÿ)],         TË†[("Planck",ğ’Ÿ)],         ğ’Ÿ),
    lâ‚("Rayleigh-Jeans", ÏƒË†[("Rayleigh-Jeans",ğ’Ÿ)], TË†[("Rayleigh-Jeans",ğ’Ÿ)], ğ’Ÿ)
)
```

+++ {"editable": true, "slideshow": {"slide_type": ""}}

$B^{\mathrm{Bayes}}$: Bayes factor
~ The Bayes factor is the ratio of the evidence for each model:
  $$\begin{aligned}
  B^{\mathrm{Bayes}} &= \frac{\eE_{\mathrm{P}}}{\eE_{\mathrm{RL}}} \,, \\
  \log B^{\mathrm{Bayes}} &= \log \eE_{\mathrm{P}} - \log \eE_{\mathrm{RL}}
  \end{aligned}$$

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
def BBayes(ğ’Ÿ, Ï€logÏƒ, Ï€logT): return Criterion(logâ„°("Planck", ğ’Ÿ, Ï€logÏƒ, Ï€logT),
                                              logâ„°("Rayleigh-Jeans", ğ’Ÿ, Ï€logÏƒ, Ï€logT))
```

+++ {"editable": true, "slideshow": {"slide_type": ""}}

$B^{\mathrm{elpd}}$: $\mathrm{elpd}$ criterion
~ The $\mathrm{elpd}$ is usually reported as-is, but since it does scale like a log probability, we can make it comparable to other criteria by defining

  $$\begin{aligned}
  \log B^{\mathrm{elpd}}
  &\coloneqq \mathrm{elpd}_{\mathrm{P}} - \mathrm{elpd}_{\mathrm{RL}} \\
  &= - \log \eE_{\mathrm{P}} + \log \eE_{\mathrm{RL}} \\
  &\quad \begin{aligned}
      \;+\, \frac{1}{L'}\sum_{j=1}^{L'} \Bigg\{
      &\log \left[
      \iint \!dTdÏƒ\; p\bigl(Î»_j, \Bspec_j' \mid T, Ïƒ, a\bigr) \, \logL_a(Ïƒ, T) \, Ï€(T)Ï€(Ïƒ)
      \right] \\
      &- \log \left[
       \iint \!dTdÏƒ\; p\bigl(Î»_j, \Bspec_j' \mid T, Ïƒ, a\bigr) \, \logL_a(Ïƒ, T) \, Ï€(T)Ï€(Ïƒ)
      \right] \Biggr\}
    \end{aligned}
  \end{aligned}$$

  In practice, a large positive value for $\log B^{\mathrm{elpd}}_{AB}$ would be interpreted as strong evidence for model $A$.

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
def Belpd(ğ’Ÿ, Ï€logÏƒ, Ï€logT): return Criterion(elpd("Planck", ğ’Ÿ, Ï€logÏƒ, Ï€logT),
                                             elpd("Rayleigh-Jeans", ğ’Ÿ, Ï€logÏƒ, Ï€logT))
```

+++ {"editable": true, "slideshow": {"slide_type": ""}}

$\underline{B}^R$: Ratio of expected risk
~ As with the $\mathrm{elpd}$, the expected risk is more commonly given directly. However since we chose $Q$ to be the negative log likelihood, it is reasonable to present it as a ratio to make it comparable with other criteria:
  :::{margin}
  Signs are flipped because our criteria are interpreted as ratios of probabilities (i.e. negative loss).
  :::
  $$\log \underline{B}^R = -R_{\mathrm{P}} + R_{\mathrm{RJ}}$$

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
def BR(ğ’Ÿ): return Criterion(-R("Planck", ğ’Ÿ), -R("Rayleigh-Jeans", ğ’Ÿ))
```

+++ {"editable": true, "slideshow": {"slide_type": ""}}

(code_uv-comparison-table)=
## Comparison table

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
df = pd.DataFrame({(f"{ğ’Ÿ.Î»min.m}â€“{ğ’Ÿ.Î»max.m}", ğ’Ÿ.L, ğ’Ÿ.B0.m, ğ’Ÿ.s.m, noise_fraction[ğ’Ÿ.B0, ğ’Ÿ.s].m):
                   {r"$\underline{B}^{\mathrm{EMD}}_{\mathrm{P,RJ}}$": Bemd(ğ’Ÿ).logratio,
                    r"$\underline{B}^R_{\mathrm{P,RJ}}$": BR(ğ’Ÿ).logratio,
                    r"$B^l_{\mathrm{P,RJ}}$": Bl(ğ’Ÿ).logratio,
                    r"$B^{\mathrm{Bayes}}_{\mathrm{P,RJ};Ï€}$": BBayes(ğ’Ÿ, Ï€1logÏƒ, Ï€1logT).logratio,
                    #r"$B^{\mathrm{Bayes}}_{\mathrm{P,RJ};Ï€_2}$": BBayes(ğ’Ÿ, Ï€2logÏƒ, Ï€2logT).logratio,
                    r"$B^{\mathrm{elpd}}_{\mathrm{P,RJ};Ï€}$": Belpd(ğ’Ÿ, Ï€1logÏƒ, Ï€1logT).logratio,
                    #r"$B^{\mathrm{elpd}}_{\mathrm{P,RJ};Ï€_2}$": Belpd(ğ’Ÿ, Ï€2logÏƒ, Ï€2logT).logratio,
                    }
                   for ğ’Ÿ in tqdm(dataset_list)})
df.columns.names=["Î»", "L", "B0", "s", "rel. Ïƒ"]
df.index.name = "Criterion"

df = df.stack(["Î»", "L"], future_stack=True)
# Put biased data before unbiased, to match figure
df = df.sort_index(axis="columns", ascending=[False, True])
```

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
# Undo the sorting along Î», so the longer wavelengths case comes first (same order as the figure)
_index = []
for i in range(0, len(df.index), 6):
    _index.extend(df.index[i+3:i+6])
    _index.extend(df.index[i:i+3])
df = df.loc[_index]
# Make a MultiIndex so the heading shows at the top instead of to the left
#df.columns = pd.MultiIndex.from_product([["noise $(s^{-1})$"], df.columns.values])
```

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
tags: [hide-input]
---
#â€¯Colorscheme for shading the table. Values will be transformed with log10;
# scheme goes from 0.01 (-2) to 100 (+2);
# See https://matplotlib.org/stable/api/_as_gen/matplotlib.colors.LinearSegmentedColormap.html#matplotlib.colors.LinearSegmentedColormap
# and https://matplotlib.org/stable/users/explain/colors/colormap-manipulation.html
shade_planck = config.figures.colors["pale"]["cyan"]
shade_rj = config.figures.colors["pale"]["red"]

cdict = {"red": [], "green": [], "blue": [], "alpha": []}
r,g,b = mpl.colors.to_rgb(shade_rj)  # Strong evidence for Rayleigh-Jeans
cdict["red"].append((0, r, r)); cdict["green"].append((0, g, g)); cdict["blue"].append((0, b, b))
cdict["alpha"].append((0, 1.0, 1.0))
r,g,b = 1,1,1  #â€¯Center
cdict["red"].append((0.5, r, r)); cdict["green"].append((0.5, g, g)); cdict["blue"].append((0.5, b, b))
cdict["alpha"].append((0.5, 0.0, 0.0))
r,g,b = mpl.colors.to_rgb(shade_planck)  # Strong evidence for Planck
cdict["red"].append((1, r, r)); cdict["green"].append((1, g, g)); cdict["blue"].append((1, b, b))
cdict["alpha"].append((1, 1.0, 1.0))

table_cmap = mpl.colors.LinearSegmentedColormap("rb_diverging", cdict)
del r,g,b
```

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
tags: [remove-cell]
---
# Format values B near 0 with 2 decimals
# B â©¾ 10000 : scientific notation (1 sig digit)
# B < 0.01  : scientific notation (1 sig digit)
def format_B_value(B: float, format):
    if B == 0:            return "0"
    elif abs(B) >= 10000: return viz.format_scientific(B, sig_digits=1, format=format)
    elif abs(B) < 0.01:   return viz.format_scientific(B, sig_digits=1, format=format)
    else:                 return f"{B:.2f}"
```

+++ {"editable": true, "slideshow": {"slide_type": ""}}

The colour scheme was picked to saturate at ratios of 30:1, which is around where [common practice](https://www.statlect.com/fundamentals-of-statistics/Jeffreys-scale) would interpret the evidence of a Bayes factor to be â€œstrongâ€ or â€œvery strongâ€.

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
tags: [remove-output]
---
df = df \
    .rename_axis(["Criterion", r"$Î» (\mathrm{Î¼m})$", "$L$"], axis="index") \
    .rename_axis(["", "$s$", "rel. $Ïƒ$"], axis="columns")
# Common options
df_styled_html = df.copy().style \
    .background_gradient(table_cmap, axis=0, vmin=-1.5, vmax=+1.5)  # logâ‚â‚€ 30 â‰ˆ 1.47
df_styled_latex = df.copy().style \
    .background_gradient(table_cmap, axis=0, vmin=-1.5, vmax=+1.5)  # logâ‚â‚€ 30 â‰ˆ 1.47
# HTML options
df_styled_html = df_styled_html \
    .format_index(lambda L: viz.format_pow2(L), axis="index", level=2, escape="latex") \
    .format_index(lambda s: viz.format_pow2(s), axis="columns", level=1, escape="html") \
    .format_index(lambda nf: f"{nf*100:2.0f}%", axis="columns", level=2) \
    .format_index(lambda B0: "$\mathcal{B}_0 > 0$" if B0>0 else "$\mathcal{B}_0 = 0$", axis="columns", level=0, escape=None) \
    .format(partial(format_B_value, format="unicode")) \
    .set_table_styles([{"selector": "td", "props": "padding: 0.5em;"},
                       {"selector": "th", "props": "text-align: center; border-bottom: 1px black"}])
df_styled_html
```

+++ {"editable": true, "slideshow": {"slide_type": ""}, "tags": ["remove-cell"]}

The cell below is meant for Jupyter Book, which doesnâ€™t render MathJax in tables.
The `html_code` variable is also used below to create the HTML code which is inserted into the paper.

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
tags: [remove-input]
---
html_code = viz.jb_fixes.display_dataframe_with_math(df_styled_html)
display(html_code)
```

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
label = "tbl_uv_criteria-comparison"
short_caption = "Comparison of different model selection criteria."
# '@' is a hack to prevent our own preprocessors from substituting \cref with {numref}
caption = r"""
\textbf{Comparison of different model selection criteria} for variations of the datasets shown in
\@cref{fig_uv-example_r-distributions}. Criteria compare the Planck model ($\MP$) against the 
Rayleigh-Jeans model ($\MRJ$) and are evaluated for different dataset sizes
($L$), different levels of noise ($s$) and different wavelength windows ($Î»$).
To allow for comparisons, all criteria have been transformed into ratios of probabilities (see \@cref{eq_def_Bemd-BR-table__emd,eq_def_Bemd-BR-table__R}).
\textbf{Values reported are the $\log_{10}$ of those ratios.}
Positive (blue shaded) values indicate evidence in favour of the Planck model, while negative (red shaded) values indicate the converse.
For example, a value of +1 is interpreted as $\MP$ being 10 times more likely than $\MRJ$,
while a value of -2 would suggest that $\MP$ is 100 times \emph{less} likely than $\MRJ$.
Noise levels are reported in two ways: $s$ is the actual value used to
generate the data,
while "rel. $Ïƒ$" reports the resulting standard deviation as a fraction of the maximum radiance within
the data window.
The \qtyrange{Â«Î»microwave_lowÂ»}{Â«Î»microwave_highÂ»}{\um} window for $Î»$ stretches into the far infrared range,
where the two models are nearly indistinguishable;
hence higher positive values are expected for zero bias, low noise, and the \qtyrange{Â«Î»infrared_lowÂ»}{Â«Î»infrared_highÂ»}{\um} window.
As in \@cref{fig_uv-example_r-distributions}, calculations were done for both positive
and null bias conditions (resp. ($\mathcal{B}_0 > 0$ and $\mathcal{B}_0 = 0$);
the former emulates a situation where neither model can fit the data perfectly.
Expressions for all criteria are given in \@cref{app_expressions-other-criteria}.
For the $\underline{B}^{\mathrm{EMD}}_{\mathrm{P,RJ}}$ criteria, we used $c=\num{Â«c_chosenÂ»}$.
Although this is the same value as we used for neuron model, it was determined through a separate calibration with different epistemic distributions.
""" \
.replace("@", "") \
.replace("Â«Î»microwave_lowÂ»", str(table_Î»window1[0].m)) \
.replace("Â«Î»microwave_highÂ»", str(table_Î»window1[1].m)) \
.replace("Â«Î»infrared_lowÂ»", str(table_Î»window2[0].m)) \
.replace("Â«Î»infrared_highÂ»", str(table_Î»window2[1].m)) \
.replace("Â«c_chosenÂ»", str(c_chosen)) \
.replace(r".\ ", ". ")
```

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
tags: [hide-input]
---
def tex2md(text):
    # Hacks
    text = text.replace(r"\textbf{Values reported are the $\log_{10}$ of those ratios.}",
                        r"__Values reported are the $\log_{10}$ of those ratios.__")
    text = text.replace(r"\cref{eq_def_Bemd-BR-table__emd,eq_def_Bemd-BR-table__R}",
                        r"{eq}`eq_def_Bemd-BR-table`")
    # "Proper" (but buggy) substitutions
    text = re.sub(r"\\emph{([^}]*?)}", r"_\1_", text)
    text = re.sub(r"\\textbf{([^}]*?)}", r"__\1__", text)
    text = re.sub(r"\\cref{([^},]+?),([^},]+?)}", r"{numref}`\1` and {numref}`\2`", text)
    text = re.sub(r"\\cref{([^}]*?)}", r"{numref}`\1`", text)
    text = re.sub(r"\\qtyrange{(\d*)}{(\d*)}{([^}]*)}", r"\1â€”\2&nbsp;$\\mathrm{\3}$", text)
    text = re.sub(r"\\um", r"\\mu m", text)
    text = re.sub(r"\\num{([^}]*?)}", r"\1", text)
    text = text.replace("~", "&nbsp;")
    text += "[[source]](#code_uv-comparison-table)"
    return text
```

+++ {"editable": true, "slideshow": {"slide_type": ""}, "tags": ["remove-cell"]}

Jupyter book has poor support for long table captions.
The solution used below is to put the caption in a sidebar.
Biggest disadvantage is that this doesnâ€™t get the standard table link and formatting.
(Possibly with lots of extra work we could emulate that ?)

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
tags: [remove-cell]
---
with open(path_output_table.with_suffix(".html"), 'w') as f:
    f.write("`````{only} html\n")
    f.write("````{margin}\n")         # Open margin
    # Workaround to get a numbered table: Put an empty list-table in the margin
    f.write(f"({label})=\n")          # Anchor for cross-references
    f.write(f":::{{list-table}} &nbsp;\n" "* - &nbsp;\n:::\n") # Empty table
    f.write("````\n")                 # Close margin
    # Workaround to associate caption & table: put them in a card
    f.write(":::{card} \n")           # Open card
    f.write(tex2md(caption).strip() + "\n")  # Add caption
    f.write("```{raw} html\n" + html_code._repr_html_() + "```\n")  # Table content
    f.write(":::\n")                  # Close card
    f.write("`````\n")
```

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
tags: [remove-cell]
---
# LaTeX options
(df_styled_latex
    .format_index(lambda B0: "$\mathcal{B}_0 > 0$" if B0>0 else "$\mathcal{B}_0 = 0$",
                  escape=None, axis="columns", level=0)
    .format_index(lambda s: viz.format_pow2(s, format='$latex$'),
                  escape="latex", axis="columns", level=1)
    .format_index(lambda nf: f"{nf*100:2.0f}\%",
                  escape="latex", axis="columns", level=2) \
    .format_index(lambda L: viz.format_pow2(L, format='$latex$'),
                  escape="latex", axis="index", level=2) \
    .format_index(escape=None, axis="index")
    .format(partial(format_B_value, format='siunitx'))
);
# # Escape the # in the index name (escape='latex' only applies to index values, not headers)
# names = list(df_styled_latex.index.names)
# names[2] = r'\# samples ($L$)'
# df_styled_latex.index.names = names
```

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
tags: [remove-cell]
---
with open(path_output_table.with_suffix(".latex", 'w') as f:
    latex_code = df_styled_latex.to_latex(
            label=label, caption=(caption, short_caption),
            environment="table*",
            column_format="ccrSSSSSS", multirow_align="c", multicol_align="c",
            hrules=True, clines="skip-last;data",
            sparse_index=True, sparse_columns=True, siunitx=True,
            convert_css=True,
        )
    # There is no column equivalent to clines, so we add the \cmidrules ourselves
    i = latex_code.index("\\\\") + 3 # Position after the first header (i.e. the bias header)
    latex_code = latex_code[:i] + "\\cmidrule(rl){4-6} \\cmidrule(rl){7-9}\n" + latex_code[i:]
    # Remove the last two clines: they are superfluous with bottomrule
    for m in list(re.finditer(r"\\cline{.*?}", latex_code))[-1::][::-1]:
        latex_code = latex_code[:m.start()] + latex_code[m.end():]
    # Replace clines with cmidrules (probably a bug: if we already use booktabs, cmidrule is much better)
    latex_code = latex_code.replace("cline", "cmidrule")
    f.write("```{raw} latex\n")
    f.write(latex_code)
    f.write("```")
```

+++ {"editable": true, "slideshow": {"slide_type": ""}, "tags": ["remove-cell"]}

## Exported notebook variables

These can be inserted into other pages.

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
tags: [remove-cell]
---
glue("c_chosen", f"${viz.format_pow2(c_chosen, 'latex')}$", display=True)

glue("Ï€Ïƒdesc", r"$Ïƒ$ follows a log uniform distribution between "
               f"2^{{{int(Ï€1logÏƒ.args[0])}}} and 2^{{{int(Ï€1logÏƒ.args[1])}}}")
glue("Ï€Tdesc", r"$T$ follows a log uniform between "
               f"{exp(Ï€1logT.args[0]).round().astype(int)} "
               f"and {exp(Ï€1logT.args[1]).round().astype(int)}")

#glue("sunits") defined in Ex_UV
glue("table_s1", viz.format_pow2(s_list[0].m, format='latex'), raw_myst=True, raw_latex=True)
glue("table_s2", viz.format_pow2(s_list[1].m, format='latex'), raw_myst=True, raw_latex=True)
glue("table_s3", viz.format_pow2(s_list[2].m, format='latex'), raw_myst=True, raw_latex=True)
glue("table_L_list", f"${viz.format_pow2(L_list[0], format='latex')}$, "
                     f"${viz.format_pow2(L_list[1], format='latex')}$, and "
                     f"${viz.format_pow2(L_list[2], format='latex')}$",
     raw_myst=True, raw_latex=True)
glue("table_lambda_range1", f"{table_Î»window1[0].m}â€“{table_Î»window1[1].m} ${table_Î»window1[0].units:~L}$",
     raw_myst=True, raw_latex=True)
glue("table_lambda_range2", f"{table_Î»window2[0].m}â€“{table_Î»window2[1].m} ${table_Î»window2[0].units:~L}$",
     raw_myst=True, raw_latex=True)
# glue("table_Î»min1", **viz.formatted_quantity(table_Î»window1[0], precision=0))
# glue("table_Î»max1", **viz.formatted_quantity(table_Î»window1[1], precision=0))
# glue("table_Î»min2", **viz.formatted_quantity(table_Î»window2[0], precision=0))
# glue("table_Î»max2", **viz.formatted_quantity(table_Î»window2[1], precision=0))
glue("table_T", **viz.formatted_quantity(table_T))
glue("table_B0_mag", data_B0.m)

glue("color_RJ", color_labels.RJ, display=True)
glue("color_Planck", color_labels.Planck, display=True)
```

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
tags: [remove-input]
---
emd.utils.GitSHA()
```

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---

```
